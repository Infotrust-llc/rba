{"cells":[{"cell_type":"markdown","metadata":{"id":"5qqlVIlqywJT"},"source":["# Data Pre-processing & Feature Selection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GZM9-fB7nMvK"},"outputs":[],"source":["###########################################################################\n","#\n","#  Copyright 2021 Google Inc.\n","#\n","#  Licensed under the Apache License, Version 2.0 (the \"License\");\n","#  you may not use this file except in compliance with the License.\n","#  You may obtain a copy of the License at\n","#\n","#      https://www.apache.org/licenses/LICENSE-2.0\n","#\n","#  Unless required by applicable law or agreed to in writing, software\n","#  distributed under the License is distributed on an \"AS IS\" BASIS,\n","#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","#  See the License for the specific language governing permissions and\n","#  limitations under the License.\n","#\n","# This solution, including any related sample code or data, is made available \n","# on an “as is,” “as available,” and “with all faults” basis, solely for \n","# illustrative purposes, and without warranty or representation of any kind. \n","# This solution is experimental, unsupported and provided solely for your \n","# convenience. Your use of it is subject to your agreements with Google, as \n","# applicable, and may constitute a beta feature as defined under those \n","# agreements.  To the extent that you make any data available to Google in \n","# connection with your use of the solution, you represent and warrant that you \n","# have all necessary and appropriate rights, consents and permissions to permit \n","# Google to use and process that data.  By using any portion of this solution, \n","# you acknowledge, assume and accept all risks, known and unknown, associated \n","# with its usage, including with respect to your deployment of any portion of \n","# this solution in your systems, or usage in connection with your business, \n","# if at all.\n","###########################################################################"]},{"cell_type":"markdown","metadata":{"id":"uh6JyZGLcEgE"},"source":["## 0) Dependencies"]},{"cell_type":"code","source":["################################################################################\n","######################### CHANGE BQ PROJECT NAME BELOW #########################\n","################################################################################\n","\n","project_name = '' #add proj name"],"metadata":{"id":"ajeI2JqEf6Vd"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4aupXJjFcHkq"},"outputs":[],"source":["# Google credentials authentication libraries\n","from google.colab import auth\n","auth.authenticate_user()\n","\n","import sys\n","\n","# data processing libraries\n","import numpy as np\n","from numpy.core.numeric import NaN\n","import datetime\n","import pandas as pd\n","import pandas_gbq\n","\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","!pip install boruta #boruta for feature selection\n","from boruta import BorutaPy\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.linear_model import LinearRegression\n","from sklearn.tree import DecisionTreeClassifier\n","\n","# modeling and metrics\n","from scipy.optimize import least_squares\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","from statsmodels.tools.tools import add_constant\n","import statsmodels.api as sm\n","\n","\n","import itertools\n","from scipy.stats.stats import pearsonr\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = [10, 5] #change size of plot\n","import seaborn as sns\n","import plotly.express as px\n","\n","# BigQuery Magics\n","'''\n","BigQuery magics are used to run BigQuery SQL queries in a python environment.\n","These queries can also be run in the BigQuery UI\n","'''\n","\n","from google.cloud import bigquery\n","from google.cloud.bigquery import magics\n","\n","magics.context.project = project_name  #update your project name \n","\n","client = bigquery.Client(project=magics.context.project)"]},{"cell_type":"markdown","metadata":{"id":"N4W_dqFdtODF"},"source":["## 1) Import dataset"]},{"cell_type":"code","source":["################################################################################\n","######################### CHANGE BQ PROJECT NAME BELOW #########################\n","################################################################################"],"metadata":{"id":"pkdnVtHnYAlr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MiR2vmeoyeLw"},"outputs":[],"source":["%%bigquery df\n","SELECT *\n","FROM `.RBA_demo.SAMPLE_DATA`; #update with project name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kawnZxuyw0L3"},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","source":["The first step is to remove variables that won't be used in the model. In this example, we remove columns like geo which is consistent across the dataset and aggregated media such as total clicks across DSPs.\n"],"metadata":{"id":"xU4CTzXPET0S"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lKXZi_hlDrj8"},"outputs":[],"source":["df.drop(columns = ['geo','x1','x2','x8','x18','x19','x20','x21','x22','x23','x24','x25'], inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Fj7Ja1d9HmE"},"outputs":[],"source":["len(df.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XhiairgY8-XL"},"outputs":[],"source":["df.describe()"]},{"cell_type":"code","source":["#Set the date as index\n","date_col = \"date\" #@param {type:\"string\"}\n","df = df.sort_values(date_col).set_index(date_col)"],"metadata":{"id":"jMUNuZnoDrto"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Option to aggregate daily data to weekly data"],"metadata":{"id":"0qnMk7hcCh3s"}},{"cell_type":"code","source":["is_daily_data = False #@param {type:\"boolean\"}\n","#if you are using weekly data, make sure this is False. If using daily data, set to True."],"metadata":{"id":"4vrz4EdICwoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if is_daily_data == False:\n","  df = df.resample('7D').sum() #aggregate daily data to weekly"],"metadata":{"id":"T7_-23QJClBS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"Bum7dqyoC8Oa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uBmArInZ00VB"},"source":["## 2) Data Cleaning"]},{"cell_type":"markdown","metadata":{"id":"1Bfetmm51Kcv"},"source":["### 2.1) Check for missing data and impute"]},{"cell_type":"markdown","source":["Check the amount of of missing values (% of total column) in the data and sort by \n","highest to lowest."],"metadata":{"id":"MCj0Ek9uEcIh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DuVFwXoa1AHY"},"outputs":[],"source":["missing_values = 100*df.isnull().sum()/len(df)\n","missing_values.sort_values(ascending = False)"]},{"cell_type":"markdown","source":["If there are any NAs in the data that should be zeros, replace those data\n","points with zero."],"metadata":{"id":"udPqb6_cEhTm"}},{"cell_type":"code","source":["df.fillna(0, inplace = True)"],"metadata":{"id":"zYqfqsqetb5_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qtcr2IYbmG4E"},"source":["## 3) Define Y (KPI column) and create initial feature set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OjJzWX6ulXYt"},"outputs":[],"source":["#Input column names for Y (ex: \"new_accounts\" or \"sales\") \n","kpi_col = \"y1\" #@param {type:\"string\"}\n","target_variable = df[kpi_col] #y variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ektWbmu5HPVk"},"outputs":[],"source":["# Create a dataframe for features (all variables except date and kpi) x variables\n","featureset_df = df[df.columns[df.columns != date_col]]\n","featureset_df = df[df.columns[df.columns != kpi_col]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXqRQTVSHGS-"},"outputs":[],"source":["featureset_df.head()"]},{"cell_type":"markdown","metadata":{"id":"YgwVdkgCYa59"},"source":["## 4) Visualize Series"]},{"cell_type":"markdown","source":["Optional:\n","\n","Visualizing each series is useful to better understand the underlying distribution of the data. This allows for examination of outliers. \n","\n","Understanding the distribution of the underlying data can also inform prior parameterization in bayesian modeling approaches later on."],"metadata":{"id":"UbMjymbzFH5W"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vNg6FK5vYeU_"},"outputs":[],"source":["for i in range(2,len(featureset_df.columns)):\n","  plt.figure()\n","  sns.kdeplot(featureset_df[featureset_df.columns[i]], label = featureset_df.columns[i], shade = True)"]},{"cell_type":"markdown","source":["## 5) Feature Creation"],"metadata":{"id":"6guckVPESzS5"}},{"cell_type":"markdown","source":["### 5.1) Check for Seasonality and add Flag"],"metadata":{"id":"Wf-gq2HQT6hl"}},{"cell_type":"markdown","source":["View the target variable as a time series plot and identify periods where data peaks.\n","\n","We also add flags for periods of peak seasonality such as Q2, Q3, and major winter holidays.\n"],"metadata":{"id":"P0YLKiyNFbBz"}},{"cell_type":"code","source":["fig = px.line(df[kpi_col])\n","fig.show()"],"metadata":{"id":"B171c0iiT-Gz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["featureset_df['Is_Q2Q3'] = (df.index.get_level_values(0).month == 4).astype(int) | (df.index.get_level_values(0).month == 5).astype(int) | (df.index.get_level_values(0).month == 6).astype(int) | (df.index.get_level_values(0).month == 7).astype(int) | (df.index.get_level_values(0).month == 8).astype(int) | (df.index.get_level_values(0).month == 9).astype(int)\n","featureset_df['Is_Holiday'] = ((df.index == '2017-11-17') | (df.index == '2017-12-22') | (df.index == '2018-11-16') | (df.index == '2018-12-21') | (df.index == '') | (df.index == ''))"],"metadata":{"id":"_x487wMOUY6T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.2) Lag, Diminishing Returns, Adstock"],"metadata":{"id":"O3AGZk-cT-zj"}},{"cell_type":"markdown","source":["We'll need to transform the raw data by applying lag, diminishing returns, and adstock returns to have it most accurately predict the target variable. \n","\n","\n","- We define lag as the impact of media on sales \"n\" days after it was served.\n","\n","- We define diminishing returns as the saturation point media will hit after a certain amount of spend, thereby becoming less effective for each additional dollar spent\n","\n","- We define adstock as the effect of media spend across a number of days\n","\n","\n","\n","\n","First, split the df into two different dataframes:\n","\n","1. Features that don't need to be transformed\n","      - Examples are: \n","          - date\n","          - target variable\n","          - control variables (seasonality, promotions, etc.)\n","\n","2. Features that do need to be transformed\n","      -  Paid media tactics \n","      -  Any other feature where there is some sort of delayed response with the target variable\n","\n","\n","Starting points for lags:\n","- If you are using daily data, the lag should at default be 14.\n","- If you are using weekly data, the lag should at default be 5.\n","\n","Others can and should be tested to determine the best lag length for your specific data.\n"],"metadata":{"id":"o8FKlBYJFk7V"}},{"cell_type":"code","source":["# Variables that do not need to be transformed\n","\n","untransformed_df = pd.concat([target_variable, featureset_df[['Is_Q2Q3','Is_Holiday']]], axis = 1) #Target variable + controls"],"metadata":{"id":"iXZAGMDmX-Vr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Variables that do need to be transformed\n","\n","#exclude dummies/controls that do not need to be transformed\n","#transformed_df = featureset_df[['feature1', 'feature2',...]]\n","\n","'''\n","Note: In this example case, almost all of the features in the featureset_df are media features.\n","As more dummy variables or other control variables are added, the user will need to \n","specify which columns should be transformed\n","'''\n","transformed_df = featureset_df.loc[:,~featureset_df.columns.isin(['Is_Q2Q3','Is_Holiday'])]"],"metadata":{"id":"SwllbKVnYXwM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 5.2.1) Create the transformation functions"],"metadata":{"id":"WfQfeIlOMRvj"}},{"cell_type":"code","source":["# This function builds the values for the diminishing returns curve, which are\n","# later applied in the transformation step\n","\n","def buildDReturnsValues(index, original_column, percent):\n","  if index == 0:\n","    return [original_column[0] * percent]\n","  else:\n","    previous_values = buildDReturnsValues(index-1, original_column, percent)\n","    previous_values.append(original_column[index] * percent + previous_values[index-1] * (1-percent))\n","    return previous_values"],"metadata":{"id":"xXVWlsWoxMeV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#This step can take several minutes\n","#This creates all combinations and then calculates the correlation between each variable and the Y variable. \n","#Returns the top 3 highest correlated features\n","\n","def create_transformations(df1, df2):\n","  columns = df2.columns\n","  sales = df1[[kpi_col]]\n","  all_data = [] \n","  for col in columns:\n","    newdf = Transformation(df2, col, True)\n","    corr_df = pd.concat([sales, newdf], axis=1)\n","    corr = abs(corr_df.corr().sort_values(kpi_col, ascending=False))\n","    new_vals= corr.iloc[1:4 , 0:1].index.tolist()\n","    data = newdf[new_vals]\n","    all_data.append(data)\n","  final_data = pd.concat(all_data,axis=1)\n","  return final_data"],"metadata":{"id":"-UyZzxVBxNGk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#This function creates every combination of Lag, Adstock (carryover), and Diminishing Returns Shape\n","#The only thing that needs to be updated is this function is the lag  \n","# If you are using weekly data, keep range(0, 4, 1). This is testing lags from 0 - 4 weeks\n","# If you are using daily data, sugestion to update  range(0, 4, 1) to  range(0, 14, 1). This will test lags from 0 - 14 days.\n","\n","def Transformation(dataframe, x, is_daily_data = is_daily_data):\n","    lag = []\n","    for i in range(0, 14 if is_daily_data else 4 , 1):\n","        data = dataframe[x].shift(i).to_frame()\n","        data.columns = [col_name+'lag'+str(i)for col_name in data.columns]\n","        lag.append(data)\n","    lag = pd.concat(lag,axis=1)\n","    lag = lag.fillna(0)\n","    dreturns = []\n","    for i in np.linspace(0.6,1.0,num=5):\n","      data = pow(lag,i)\n","      data.columns = [col_name+'dreturns'+str(i)for col_name in data.columns]\n","      dreturns.append(data)\n","    dreturns = pd.concat(dreturns,axis=1) \n","    adstock=[]\n","    #j = 0\n","    for percent in np.linspace(0.6,1.0,5):\n","        data = dreturns.copy()\n","        data.columns = [col_name+'adstock'+str(percent)for col_name in data.columns]\n","        for j in range(0, len(dreturns.columns)):\n","          data[data.columns[j]] = buildDReturnsValues(len(data[data.columns[j]])-1, data[data.columns[j]], percent)\n","        \n","        adstock.append(data)\n","        #j = j + 1\n","    adstock = pd.concat(adstock,axis=1)\n","    \n","    return adstock"],"metadata":{"id":"WuhDt4spxb0c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 5.2.1) Implement the transformations"],"metadata":{"id":"nVKeMikfMWG9"}},{"cell_type":"markdown","source":["Make sure data is correctly sorted by date before running feature selection algo.\n","\n","This is important because the algorithm takes from a previous row of the data as it evaluates the current row. Unsorted data can cause errors in resulting feature selection\n","info.\n"],"metadata":{"id":"Ru3DHBCV-kR3"}},{"cell_type":"code","source":["transformed_df = transformed_df.sort_values(date_col)\n","sys.setrecursionlimit(len(transformed_df.index)+100)\n","\n","transformed_df = create_transformations(untransformed_df, transformed_df)\n","transformed_df.head()"],"metadata":{"id":"y5G4HnsCxcco"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6) Feature Selection"],"metadata":{"id":"u3DlsBG_Izo_"}},{"cell_type":"markdown","source":["For feature selection we employ the Boruta algorithm.[(More information here)](https://towardsdatascience.com/boruta-explained-the-way-i-wish-someone-explained-it-to-me-4489d70e154a\n",")\n","\n","This algorithm will tell you the rank of each feature and whether or not to keep a varaible in the model (i.e. Keep  = True/False). The goal of RBA is to optimize across all paid digital media tactics, therefore select the top ranking feature for each group of features (whether or not the algorithm tells you to keep the feature).\n"],"metadata":{"id":"R6VoaFdyHRsV"}},{"cell_type":"code","source":["# Specifiying the target and x variables\n","y = target_variable\n","x = transformed_df #update with transformed features"],"metadata":{"id":"yXWDCqcVF0KX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define random forest classifier\n","forest = RandomForestRegressor(n_jobs=-1, max_depth=5)\n","forest.fit(x, y)"],"metadata":{"id":"fk4eshicHtQZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define Boruta feature selection method\n","feat_selector = BorutaPy(forest, n_estimators='auto', verbose=2, random_state=1)\n","\n","# find all relevant features\n","feat_selector.fit(np.array(x), np.array(y))\n","\n","# check selected features\n","feat_selector.support_\n","\n","# check ranking of features\n","feat_selector.ranking_\n","\n","# call transform() on X to filter it down to selected features\n","X_filtered = feat_selector.transform(np.array(x))"],"metadata":{"id":"9B8sMMKWHwOm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Select the top ranking variable for each group of variables. \n","feature_ranks = list(zip(x.columns, \n","                         feat_selector.ranking_, \n","                         feat_selector.support_))\n","\n","# iterate through and print out the results\n","for feat in feature_ranks:\n","    print('{:<25}, Rank: {},  Keep: {}'.format(feat[0], feat[1], feat[2]))"],"metadata":{"id":"Kax9LfLYH44N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Reduce the overall dataset to just selected features using the ranking\n","from the Boruta output, and save to a dataframe"],"metadata":{"id":"8xM0Uhm3HxaL"}},{"cell_type":"code","source":["selected_featureset_df = transformed_df[['x3lag12dreturns0.8adstock1.0',\n","'x4lag0dreturns1.0adstock1.0',\n","'x5lag11dreturns0.9adstock0.6',\n","'x6lag0dreturns1.0adstock1.0',\n","'x7lag12dreturns1.0adstock0.9',\n","'x9lag0dreturns0.7adstock1.0',\n","'x10lag0dreturns0.9adstock1.0',\n","'x11lag11dreturns1.0adstock0.7',\n","'x12lag0dreturns0.9adstock1.0',\n","'x13lag12dreturns0.9adstock1.0',\n","'x14lag0dreturns1.0adstock1.0',\n","'x15lag12dreturns0.9adstock1.0',\n","'x16lag12dreturns0.6adstock0.7',\n","'x17lag0dreturns0.7adstock1.0',\n","'x26lag12dreturns0.6adstock0.6',\n","'x27lag0dreturns1.0adstock0.9',\n","'x28lag9dreturns0.8adstock0.6',\n","'x29lag12dreturns0.6adstock0.8',\n","'x30lag0dreturns1.0adstock1.0',\n","'x31lag0dreturns1.0adstock0.8',\n","'x32lag0dreturns0.6adstock0.6',\n","'x33lag12dreturns0.6adstock0.6',\n","'x34lag13dreturns1.0adstock0.6',\n","'x35lag12dreturns0.6adstock0.9',\n","'x36lag8dreturns1.0adstock0.6',\n","'x37lag12dreturns0.6adstock0.8',\n","'x38lag0dreturns0.7adstock0.9',\n","'x39lag12dreturns1.0adstock0.8',\n","'x40lag12dreturns1.0adstock0.6',\n","'x41lag12dreturns0.6adstock1.0',\n","'x42lag3dreturns1.0adstock0.6',\n","'x43lag4dreturns0.6adstock0.9',\n","'x44lag11dreturns0.6adstock0.8',\n","'x45lag0dreturns0.6adstock0.7',\n","'x46lag0dreturns0.6adstock0.6'\n","]]"],"metadata":{"id":"mulsX0HWRQIU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# add back in the untransformed control variables to the featureset\n","selected_featureset_df = pd.concat(\n","    [selected_featureset_df,untransformed_df[untransformed_df.columns[untransformed_df.columns != kpi_col]]],\n","    axis = 1)"],"metadata":{"id":"hlhdfRq3qExp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4rEpk2EWYuRW"},"source":["## 7) Feature Scaling"]},{"cell_type":"markdown","metadata":{"id":"6ojhRBywZJCY"},"source":["### 7.1) Feature Scaling"]},{"cell_type":"markdown","source":["The default method of standardization utilizes Standard Scaler, which takes in\n","input data and transforms so that the output has mean 0 and standard deviation of 1\n","across all features.\n","\n","\n","Alternative methods of feature scaling include square-root transformation,\n","de-meaning, natural log transformations, Min-Max Scalers, or normalization"],"metadata":{"id":"GYhi4IjlH9Iz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lsh3UTusZZeR"},"outputs":[],"source":["scaler = StandardScaler()\n","standardized_transform = scaler.fit_transform(selected_featureset_df)\n","selected_featureset_df = pd.DataFrame(standardized_transform, columns = selected_featureset_df.columns)"]},{"cell_type":"markdown","source":["Option to review visuals of the data. After the data is standardized the distributions may take on a more normal shape."],"metadata":{"id":"Jd4Jx-VgIFV4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"IapNWY0XZc9r"},"outputs":[],"source":["'''\n","for i in range(0,len(X_transform_stand.columns)):\n","  plt.figure()\n","  sns.kdeplot(X_transform_stand[X_transform_stand.columns[i]], label = X_transform_stand.columns[i], shade = True)\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6N4UP5sjd0p"},"outputs":[],"source":["selected_featureset_df.head()"]},{"cell_type":"markdown","metadata":{"id":"LE0Prw-gmXBs"},"source":["## 8) Handle Multicollinearity (reduce feature set)"]},{"cell_type":"markdown","source":["1. Print a correlation heatmap to visualize correlations across feature set\n","2. Run variance inflation factor analysis and output results to flag multicollinearity above specified threshold"],"metadata":{"id":"aDrFJIAxIJ2h"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8onrNK_JZsZ4"},"outputs":[],"source":["correl = selected_featureset_df.corr()\n","\n","# Getting the Upper Triangle of the co-relation matrix\n","matrix = np.triu(correl)\n","\n","# using the upper triangle matrix as mask \n","sns.heatmap(correl, mask=matrix)"]},{"cell_type":"markdown","source":["Run VIF analysis and flag values greater than 10.\n","\n","Industry best practice flags values above 10 as an extreme violation of regression model assumptions. [(Reference)](https://en.wikipedia.org/wiki/Variance_inflation_factor)\n"],"metadata":{"id":"7QQTR1g1N839"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TD6FJ6SDn-ic"},"outputs":[],"source":["vif = add_constant(selected_featureset_df)\n","\n","# loop to calculate the VIF for each X \n","vif = pd.Series([variance_inflation_factor(vif.values, i) \n","      for i in range(vif.shape[1])], \n","      index=vif.columns) "]},{"cell_type":"code","source":["# processing to output VIF results as a dataframe \n","vif_df=vif.to_frame().reset_index()\n","\n","vif_df.columns = ['feature', 'vif']\n","vif_df=vif_df.replace([np.inf], np.nan) # replace inf calculations as missing and zero fill \n","vif_df=vif_df.fillna(0).sort_values(by=\"vif\", ascending=False)"],"metadata":{"id":"bbr437wVwIoc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vif_df.reset_index(inplace = True)\n","vif_df"],"metadata":{"id":"F6fSf3Hyw1gw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Drop the highest VIF features and print the high collinearity columns in a list"],"metadata":{"id":"_NLieHb1OH3q"}},{"cell_type":"code","source":["high_collinearity_columns = vif_df.feature[vif_df['vif'] >= 10].to_list()\n","high_collinearity_columns"],"metadata":{"id":"QhmaytzMZJy7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Drop 1 variable at a time (start with the highest VIF) and re-run the VIF cell to re-check multicollinearity. This will allow the user to preserve as many features in the model as possible."],"metadata":{"id":"OwJ2RObPOL0Y"}},{"cell_type":"code","source":["cols_to_drop = []\n","while vif_df.vif[1] >= 10:\n","  if vif_df.vif[1] >= 10:\n","    cols_to_drop.append(vif_df.feature[1])\n","    selected_featureset_df.drop(columns = vif_df.feature[1],inplace = True) \n","    vif = add_constant(selected_featureset_df)\n","  # loop to calculate the VIF for each X \n","    vif = pd.Series([variance_inflation_factor(vif.values, i) \n","    for i in range(vif.shape[1])], index=vif.columns) \n","    # processing to output VIF results as a dataframe \n","    vif_df=vif.to_frame().reset_index()\n","    vif_df.columns = ['feature', 'vif']\n","    vif_df=vif_df.replace([np.inf], np.nan) # replace inf calculations as missing and zero fill \n","    vif_df=vif_df.fillna(0).sort_values(by=\"vif\", ascending=False)\n","    vif_df.reset_index(inplace = True)"],"metadata":{"id":"1pJIafpZXs5B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cols_to_drop"],"metadata":{"id":"WFEa0NTUY9W_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["selected_featureset_df.columns"],"metadata":{"id":"xmOql8Li9IeX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(selected_featureset_df.columns)"],"metadata":{"id":"g3hI1pqZM02U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace the decimal points with underscores so that data can be exported to BQ\n","selected_featureset_df.columns = selected_featureset_df.columns.str.replace(\".\",\"_\")"],"metadata":{"id":"-ElFNnOWVU39"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yc7SmJGmDjPr"},"source":["## 9) Export Final Dataset"]},{"cell_type":"code","source":["df[df.columns[df.columns.isin(parse_final_column_features(selected_featureset_df.columns))]]"],"metadata":{"id":"cb6CRk28b5FT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["untransformed_df[untransformed_df.columns[untransformed_df.columns.isin(parse_final_column_features(selected_featureset_df.columns))]]"],"metadata":{"id":"MTBZtxYvl02u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.1) Trim the final dataset according to lag"],"metadata":{"id":"1qVnQK-lbOzl"}},{"cell_type":"code","source":["final_df = selected_featureset_df\n","final_df[kpi_col] = target_variable.reset_index()[kpi_col]"],"metadata":{"id":"QwvhPcKQwGwd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_df[date_col] = df.index #add back in the date as a separate column from the index"],"metadata":{"id":"rOlFNnE7EHpR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Trim the start of your dataset to correspond with the max lag\n","(for example: if max lag is 4 weeks, trim the first 4 weeks off of the data)"],"metadata":{"id":"yoNruntOOVLJ"}},{"cell_type":"code","source":["max_lag = 13\n","final_df = final_df[max_lag:]\n","final_df.reset_index(inplace = True)\n","final_df.drop(columns = 'index',inplace = True)"],"metadata":{"id":"7OfrmYucawBG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["################################################################################\n","######################### CHANGE BQ PROJECT NAME BELOW #########################\n","################################################################################"],"metadata":{"id":"hJ57QyELbSUA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["destination_project_id = \"\" #@param\n","destination_dataset = \"RBA_demo\" #@param\n","destination_table = \"cleaned_data\" #@param\n","dataset_table = destination_dataset+\".\"+destination_table\n","\n","final_df.to_gbq(dataset_table, \n","                 destination_project_id,\n","                 chunksize=None, \n","                 if_exists='replace'\n","                 )"],"metadata":{"id":"Eg4oduVkv6as"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 9.2) Prepare the data for optimization tool"],"metadata":{"id":"g-F36U_NmYEc"}},{"cell_type":"markdown","source":["The budget optimization tool requires the model features in their original un-transformed state as an input. The following function pulls the names of the required columns and collects the data from the relevant dataframes."],"metadata":{"id":"ifTaMFKV2UQt"}},{"cell_type":"code","source":["def parse_final_column_features(columns):\n","  final = []\n","  for col in columns:\n","    splitList = col[::-1].split('gal', 1)\n","    parsed = (splitList[1] if len(splitList) > 1 else splitList[0])\n","    final.append(parsed[::-1])\n","  return final"],"metadata":{"id":"R_IrGN6Amemp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer_df = df[df.columns[df.columns.isin(parse_final_column_features(selected_featureset_df.columns))]]\n","optimizer_df = optimizer_df.merge(untransformed_df[untransformed_df.columns[untransformed_df.columns.isin(parse_final_column_features(selected_featureset_df.columns)) & (untransformed_df.columns != kpi_col)]],how = 'inner',on = ['date'])"],"metadata":{"id":"xndQAgwomioj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["destination_project_id = \"\" #@param\n","destination_dataset = \"RBA_demo\" #@param\n","destination_table = \"optimizer_data\" #@param\n","dataset_table = destination_dataset+\".\"+destination_table\n","\n","optimizer_df.to_gbq(dataset_table, \n","                 destination_project_id,\n","                 chunksize=None, \n","                 if_exists='replace'\n","                 )"],"metadata":{"id":"wnH1inxh0bjR"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1rTRXzEp5Y68lKPRF0B24D6UfUZF5BIV-","timestamp":1626453655786},{"file_id":"1a9EJrqES9Vxyp9yOvevTKqE_vqFO66AI","timestamp":1626368566392}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":0}